
% \vspace{-10mm}
\section{Experiments}
\label{sec:exp}

We report on extensive experiments aimed at evaluating the performance of  \ClusterEA{}.
\input{src/tables/dataset_stat}

\vspace{-2mm}
\subsection{Experimental Settings}\label{sec:exp_setting}
\input{src/tables/ids_results}

\noindent
\textbf{Datasets.}
We conduct experiments on datasets with different sizes from two cross-lingual EA benchmarks, i.e., IDS~\cite{OpenEA2020VLDB} and DBP1M~\cite{LargeEA22}. 
\begin{itemize}[topsep=0pt,itemsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=*]
\item 
\emph{IDS} contains four cross-lingual datasets, i.e., English and French (IDS15K$_{EN-FR}$ and IDS100K$_{EN-FR}$), and English and German (IDS15K$_{EN-DE}$ and IDS100K$_{EN-DE}$). \MARK{These benchmarks are sampled with consideration of keeping the properties (e.g., degree distribution) consistent with their source KGs.} We use the latest 2.0 version of IDS, where the URIs of entities are encoded to avoid possible name bias.
\item \emph{DBP1M} is the largest cross-lingual benchmark for EA. It contains two large-scale cross-lingual datasets extracted from DBpedia~\cite{DBPedia}, i.e., English and French (DBP1M$_{EN-FR}$), and English and German (DBP1M$_{EN-DE}$). 
However, DBP1M is biased with name information. To be specific, part of the entities in inter-language links (ILLs) does not occur in the two KGs. We thus remove those ILLs to resolve the name bias issue while retaining all the triples.
\end{itemize}

Following previous works, we use 30\% of each dataset as seed alignment and use 70\% of it to test the EA performance.
As can be seen, we consider both degree distribution issue~\cite{RSN19, OpenEA2020VLDB} and the name bias issue~\cite{AttrGNN20} when selecting benchmarks, which meets the requirements of real-world applications.
Table \ref{tb:dataset} lists the detailed information of datasets.

\noindent
\textbf{Evaluation metric.}
We use the widely-adopted \HitNFull{} (\HitN{}), Mean Reciprocal Rank (\MRR{}) to evaluate the accuracy of \ClusterEA{}~\cite{MTransE17,IPTransE17, GCN-Align18, KECG19, RREA20, DualAMN21, EASY21}. Here, for \HitN{}, $N$=1, 10.
Higher \HitN{} and \MRR{} indicate better performance.
In addition, we use running time and maximum GPU Memory cost (\emph{Mem.}) to evaluate the scalability of \ClusterEA{}. Specifically, running time is measured in seconds, and \emph{Mem.} is measured in Gigabytes.
% following ~\cite{LargeEA22}, running time (in seconds), and the maximum GPU Memory cost (\emph{Mem.} for short, in Gigabytes) are included for evaluating the scalability of \ClusterEA{}.
% Here, the running time means the training time of every EA approach.
% We do not consider the test time. Specifically, it is common for EA methods to use the same approach for evaluating the EA results in the test set, resulting in the similar test time of different approaches. 

\noindent
\textbf{Baselines.}
\MARK{
We only compare \ClusterEA{} with structure-only methods. If a baseline contains side information components, we remove them in order to guarantee a fair comparison
~\cite{DualAMN21, EVA20, RREA20, HyperKA20, OpenEA2020VLDB, LargeEA22}.  All the baselines are enhanced with CSLS (\SparseCSLS{} for large-scale datasets) before evaluation if possible. The implementation details and parameter settings of \ClusterEA{} and all baselines are provided in Appendix~\ref{app:details}.
% We divide the compared baselines into two major categories :
% (i) \emph{Non-scalable} ones including GCNAlign, RREA, and Dual-AMN; (ii) \emph{Scalable} ones indicating other baselines mentioned above. 
Considering the scalability of models, we divide the compared baselines into two major categories, as listed below:
% We compare \ClusterEA{} with the following representative EA models.
% \begin{itemize}[topsep=0pt,itemsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=*]
%     \item  \emph{GCNAlign}~\cite{GCN-Align18}: it is the first GNN based EA model. We re-implement GCNAlign by removing attribute features (as the side information component) from it.
%     \item \emph{LargeEA}~\cite{LargeEA22}: it is the first EA framework that focuses on scalability by training multiple EA models on mini-batches generated by a rule-based strategy. We reproduce LargeEA by removing the name channel entirely, including the name-based data augmentation. Excepted for the two variants introduced in~\cite{LargeEA22}, 
%     we provide a new variant \emph{LargeEA-D} that incorporates recently proposed EA model Dual-AMN.
%     \item \emph{RREA}~\cite{RREA20}: it is a GNN-based EA model that leverages relational reflection transformation to obtain relation-specific embeddings for each entity. It is used as the default of LargeEA~\cite{LargeEA22}.
%     \item \emph{Dual-AMN}~\cite{DualAMN21}: it is a SOTA EA model that encomprasses  Simplified Relational Attention Layer and Proxy Matching Attention Layer for modeling both intra-graph and cross-graph relations.
%     \item \emph{Stochastic training (Section~\ref{sec:mini-batch-training}) variant of GNN models}, that incorporates EA models with Stochastic training, including \emph{GCNAlign-S} for GCNAlign~\cite{GCN-Align18}, \emph{RREA-S} for RREA~\cite{RREA20} and \emph{Dual-AMN-S} for Dual-AMN~\cite{DualAMN21}.
% \end{itemize}
\begin{itemize}[topsep=0pt,itemsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=*]
    \item \emph{Non-scalable baselines} that includes 
            (i) \emph{GCNAlign}~\cite{GCN-Align18}, the first GNN-based EA model; 
            (ii) \emph{RREA}~\cite{RREA20}, a GNN-based EA model that leverages relational reflection transformation to obtain relation-specific embeddings for each entity. It is used as the default of LargeEA~\cite{LargeEA22}; and
            (iii) \emph{Dual-AMN}~\cite{DualAMN21}, a SOTA EA model that encomprasses Simplified Relational Attention Layer and Proxy Matching Attention Layer for modeling both intra-graph and cross-graph relations.
    \item \emph{Scalable baselines} that includes 
            (i) \emph{LargeEA}~\cite{LargeEA22}, the first EA framework that focuses on scalability by training multiple EA models on mini-batches generated by a rule-based strategy. Excepted for the two variants introduced in~\cite{LargeEA22}, 
            we provide a new variant \emph{LargeEA-D} that incorporates recently proposed EA model Dual-AMN; and
            (ii) \emph{Stochastic training (Section~\ref{sec:mini-batch-training}) variant of GNN models}, that incorporates EA models with Stochastic training, including \emph{GCNAlign-S} for GCNAlign~\cite{GCN-Align18}, \emph{RREA-S} for RREA~\cite{RREA20} and \emph{Dual-AMN-S} for Dual-AMN~\cite{DualAMN21}.
\end{itemize}
}


\noindent
\textbf{Variants of \ClusterEA{}.}
Since \ClusterEA{} is designed to be integrated with GNN-based EA models, we present three versions of \ClusterEA{}, i.e.,  \ClusterEA{-G} that includes GCNAlign, \ClusterEA{-R} that incorporates RREA and  \ClusterEA{-D} that incorporates Dual-AMN. Specifically, we treat \ClusterEA{-D} as the default setting of \ClusterEA{}. 
% \MARK{Our current implementation includes three representative GNN-based EA models, including (i) GCN-Align~\cite{GCN-Align18}, the first GNN-based EA model; (ii) RREA~\cite{RREA20}, which is adopted by the first scalable EA framework LargeEA~\cite{LargeEA22}; and (iii) Dual-AMN~\cite{DualAMN21}, current state-of-the-art structure-only EA model.}

% \vspace*{-4mm}
\subsection{Overall Results}
\input{src/tables/dbp1m_results}

\noindent
\textbf{Performance on IDS.}\label{sec:exp_ids}
Table~\ref{exp:overall_15K_100K} summarizes the EA performance on IDS15K and IDS100K.
% \noindent
% \textbf{Accuracy Evaluations.}
% We first focus on the \textbf{accuracy} for all the variants of \ClusterEA{} and its competitors.
First,  \ClusterEA{} improves \HitOne{} by   $3.3\%-29.7\%$ compared with the non-scalable methods (GCNAlign, RREA, Dual-AMN), and improves it by $3.2\%-42.3\%$ compared with the scalable ones. These validate the accuracy of \ClusterEA{}.
% 3.3 29.7 4.7 37.9
% 4.0 20.8 3.2 39.2
% 4.9 24.3 6.0 34.7
% 4.4 22.7 4.9 42.3 
% 
Moreover, all variants of \ClusterEA{} perform better than the structure-only models in terms of \HitOne{}. It validates the superiority of the way how we enhance the output compared with CSLS, which is the default setting for most recent EA models~\cite{EVA20,EASY21, MRAEA20,RREA20, OpenEA2020VLDB, DualAMN21} to enhance the output similarity matrix. 

Second, it can be seen that the accuracy of LargeEA variants is significantly reduced compared with their corresponding original models. This is because LargeEA discards structure information during its mini-batch training process. Unlike the LargeEA variants, the accuracy of Stochastic Training variants only drops slightly compared with the original non-scalable models. This verifies that the Stochastic training process can minimize the structure information loss (cf. Section~\ref{sec:mini-batch-training}). 

Third, as observed,  \HitTen{} of some non-scalable models (RREA, Dual-AMN) is higher than that of \ClusterEA{}. This is mainly because: (i) the information loss during \ClusterEA{}'s Stochastic training; and (ii) the incompleteness of global similarity normalization (to be detailed in Section~\ref{sec:ablation}). Nevertheless, \HitOne{} is the most representative indicator for evaluating the accuracy of EA since higher \HitOne{} directly indicates more correct proportion of aligned entities. Thus, achieving the highest \HitOne{} among all baselines is sufficiently to confirm the high performance of \ClusterEA{}.
%considering that \HitOne{} represents the correct proportion of aligned entities, it is the most representative indicator for evaluating the accuracy. It would be an acceptable trade-off for better \HitOne{}.

Finally, the experimental results show that the training of all LargeEA variants is faster than that of other variants of the corresponding models. This is because LargeEA omits most information of graph structure, where less data is subjected to the training process. However, in this case, the running time of \ClusterEA{} is still comparable with LargeEA.


\noindent
\textbf{Performance on DBP1M.}
Table~\ref{exp:overall_1M} reports the EA performance of \ClusterEA{} and its competitors on DBP1M. Note that we do not report the results of the non-scalable models because it is infeasible to perform their training phases on DBP1M due to extensive GPU memory usage.
We observe that the accuracy of all the variants of \ClusterEA{} on DBP1M  is higher than those of LargeEA. Specifically, \HitOne{} is improved by $5.7\%-23.0\%$ and $6.8\%-25.4\%$ on DBP1M$_{EN-FR}$ and DBP1M$_{EN-DE}$, respectively.
 
% G 4.0 2.4
% R 4.9 4.5
% D 5.7 6.8
Next, \MARK{compared with each Stochastic training variants of the corresponding incorporated model of \ClusterEA{}, \ClusterEA{} brings about $4.0\%-5.7\%$ and $2.4\%-6.8\%$ absolute improvement in \HitOne{} on DBP1M$_{EN-FR}$ and DBP1M$_{EN-DE}$ datasets, respectively. As the expressiveness of the model improves, \ClusterEA{} also provides more improvements. 
% Moreover, as observed, \ClusterEA{} is able to outperform baselines without using GNN model.
% This is because \ClusterEA{} optimizes the similarity computation via proposing a novel similarity matrix.
}
%This validates the importance of enhancing the similarity matrix and the superiority of the way of \ClusterEA{} enhancing the similarity matrix. 

% Since the Stochastic training variants are treated as a sub-module in \ClusterEA{}. 
Finally, all the scalable variants incorporating GCNAlign variants produce poor EA results. Specifically, GCNAlign's model cannot be directly applied to large-scale datasets due to its insufficient expressive ability.
\MARK{Note that \ClusterEA{} not only outperforms baselines in terms of accuracy but also achieves comparable performance with them in terms of both Mem. and running time.}
\MARK{Overall, \ClusterEA{} is able to scale-up the GNN-based EA models while enhancing \HitOne{} by at most $8\times$
%the 
%not only successfully scale-up the GNN-based EA models but also enhance the performance with up to $8\times$ improvement in terms of \HitOne{} 
compared with the state-of-the-arts. More experimental results of scalability are provided in  Appendix~\ref{app:scalability} due to the space limitation.}



\begin{figure*}[t]
\centering
\includegraphics[width=.8\textwidth]{figs/icon-1.eps}\\
\vspace*{-4mm}
\subfigure[$\rm IDS15K_{EN-FR}$]{
 \includegraphics[width=1.7in]{figs/partition-k-small-fr.eps}
 \label{fig:partition-k-small-fr}
}\hspace{-3mm}
\subfigure[$\rm IDS100K_{EN-FR}$]{
 \includegraphics[width=1.7in]{figs/partition-k-medium-fr.eps}
 \label{fig:partition-k-medium-fr}
}\hspace{-3mm}
\subfigure[$\rm DBP1M_{EN-FR}$]{
 \includegraphics[width=1.7in]{figs/partition-k-large-fr.eps}
 \label{fig:partition-k-large-fr}
}\hspace{-3mm}
\subfigure[$\rm DBP1M_{EN-FR}$]{
 \includegraphics[width=1.67in]{figs/scalability-partition-en-fr-2_1.eps}
 \label{fig:scalability-p-fr}
}\\
\vspace*{-6mm}
\subfigure[$\rm IDS15K_{EN-DE}$]{
 \includegraphics[width=1.7in]{figs/partition-k-small-de.eps}
 \label{fig:partition-k-small-de}
}\hspace{-3mm}
\subfigure[$\rm IDS100K_{EN-DE}$]{
 \includegraphics[width=1.7in]{figs/partition-k-medium-de.eps}
 \label{fig:partition-k-medium-de}
}\hspace{-3mm}
\subfigure[$\rm DBP1M_{EN-DE}$]{
 \includegraphics[width=1.7in]{figs/partition-k-large-de.eps}
 \label{fig:partition-k-large-de}
}\hspace{-3mm}
\subfigure[$\rm DBP1M_{EN-DE}$]{
 \includegraphics[width=1.67in]{figs/scalability-partition-en-de-2_1.eps}
 \label{fig:scalability-p-de}
}
\vspace{-4mm}
\caption{Comparison results of different batch sampling strategies}
\vspace{-3mm}
\label{fig:partition-exp}
\end{figure*}

\input{src/tables/ablation}

\subsection{Ablation Study}\label{sec:ablation}
In ablatoin study, we remove 
each component of \ClusterEA{} and report \HitOne{}, \HitTen{}, and \MRR{} Table~\ref{exp:abl}.
%conduct ablation studies on both DBP1M$_{EN-FR}$ and  DBP1M$_{EN-DE}$ and report the results in Table~\ref{exp:abl}.
%from IDS and DBP1M benchmarks respectively,
%We report \HitOne{}, \HitTen{}, and $MRR$ after removing each component of \ClusterEA{}.
First, after removing the \SparseCSLS{} component, the accuracy of \ClusterEA{} drops. This verifies that the \SparseCSLS{} normalization indeed resolve the geometric problems of global similarity on large-scale EA. 
Second, after removing the global similarity, \HitOne{} of \ClusterEA{} drops while \HitTen{} raises on DBP1M$_{EN-FR}$. The fluctuation on \HitTen{} may be due to the incompleteness of global similarity normalization
that disturbs the final similarity matrix. 
Specifically, 
the local similarity is normalized into nearly permutation matrices with Sinkhorn iteration, where most values of one row are close to zero. 
When fusing local and global similarity matrices, elements that are not top-1 in the local matrix will be biased towards the value of the global matrix, which is only partially normalized.
This causes the disturbance.
However,
the incompleteness of normalization does not degrade \HitOne{}. This is because the value of one row in $\mathcal{M}_{L}$ that is correctly aligned will be normalized into a higher value, providing resistance to the global matrix disturbance. 
Third, after removing the Sinkhorn iteration,  the accuracy of \ClusterEA{} drops significantly. This verifies the importance of normalizing the similarity matrices. Finally, after removing each sampler of \Sampling{},  the accuracy of \ClusterEA{} drops on all metrics. This verifies the importance of fusing information from multi-aspects, including cross-KG information and intra-KG information.

Moreover, we observe that the \KMeans{} has less influence compared with \MetisGCN{}. This is mainly due to the following two reasons. First, \KMeans{} generally clusters entities with similar embedding vectors, where each batch still suffers from geometric problems. On the contrary,  \MetisGCN{} samples batches based on the graph neighborhood information, which is a strong constriction on the learning model. Second, we apply \MetisGCN{} in both directions. In this case, it can capture more information than \KMeans{}.
By replacing its Dual-AMN model with the GCNAlign model, the accuracy of \ClusterEA{} drops significantly on all metrics. This verifies the importance of the ability of the incorporated EA model in \ClusterEA{}.

% \vspace{-3.5mm}
\subsection{Case Study: \Sampling{} Analysis}
\label{sec:exp_minibatchgeneration}

The \Sampling{} is a vital component of \ClusterEA{}. To ensure scalability, we set the batch number $K$ such that the space cost of the normalization process does not exceed the GPU memory. We also need to guarantee that our batch sampling method can produce acceptable mini-batches under different $K$ settings.
Thus, we provide a detailed analysis on varying the batch number $K$ for different batch samplers from \Sampling{} (i.e., \MetisGCN{} and \KMeans{}). Specifically, we study how much are the mini-batches generated by one sampler acceptable as the percentage of equivalent entities that are placed into the same mini-batches (denoted as \textit{Overlap}).

Next, we report the Overlap metric and running time of the proposed sampler, where the mini-batch number $K$ of the proposed sampler is varied from 5 to 25 on IDS and is varied from 10 to 50 on DBP1M. We compare the proposed sampler with two rule-based baselines, VPS and METIS-CPS. \emph{VPS} randomly partitions seed alignments and all other entities into different mini-batches. \emph{METIS-CPS} sets the training entities with higher nodes to sample better mini-batches.
Note that both METIS-CPS and \MetisGCN{} are unidirectional. Thus, we apply these methods in both directions and report their average performance of the two directions. We report the overlap of all sampling methods on all benchmarks in Fig~\ref{fig:partition-exp}, where Figure~\ref{fig:partition-k-small-fr},~\ref{fig:partition-k-medium-fr},~\ref{fig:partition-k-large-fr},~\ref{fig:partition-k-small-de}, ~\ref{fig:partition-k-medium-de},and~\ref{fig:partition-k-large-de} are the overlap of different datasets. The results show that \KMeans{} generally outperforms two baselines on all the datasets, and its performance is stable when varying $K$. However, although better overlapped, it may result in hubness and isolation problems in mini-batches (cf. Section~\ref{sec:ablation}), thus fusing multi-aspect is essential for \ClusterEA{}. The \MetisGCN{} results in less overlapped mini-batches while still much better than the METIS-CPS. This is because METIS does not necessarily follow the guidance provided by METIS-CPS. The GNN node classification model in \MetisGCN{}, which considers the cross-entropy loss as a penalty, is forced to learn mini-batches more effectively. Since we set the training ratio to 30\%, all samplers have an overlap over 30\%, including VPS that splits mini-batches randomly.

Finally, we report the running time on DBP1M in Figure~\ref{fig:scalability-p-fr} and~\ref{fig:scalability-p-de}. Since all samplers achieve sufficiently high efficiency on IDS datasets, we do not report the running time of all samplers on IDS due to space limitations. We observe that although the result is unacceptable, VPS is the fastest sampling method. Both the proposed \MetisGCN{} and \KMeans{} are always about $2\times$ faster than the rule-based METIS-CPS when varying $K$. This is because both of them utilize machine learning models, which could be accelerated with GPU. 





